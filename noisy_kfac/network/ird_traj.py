import tensorflow as tf
import numpy as np

from ..core import MODE_REGRESSION, MODE_IRD
from ..misc.layers import dense
from .registry import register_model

def IRDTraj(main_inputs, aux_inputs, n_main, n_aux, layer_out_sizes, sampler,
        is_training, batch_norm, layer_collections, n_particles):
    """
    Build a IRD-style dense network which predicts `main_outputs`, the reward of
    the total-trajectory features in `main_inputs`. Similarly, IRDTraj predicts
    `aux_outputs`, the reward of total-trajectory features in `main_outputs`.

    `main_outputs` represent the unnormalized rewards.
    `aux_outputs` are used to compute the normalization constant Z(w_star).

    This function also constructs `ll`, the mean approximate IRD log-likelihood
    of any proxy reward weights that could produce `main_inputs`, given that the
    currently sampled weights of the neural network are the true reward weights.
    The mean is taken over all the feature vectors in `main_inputs` and each of
    the `n_particles` weights drawn from the neural network.

    Params:
      main_input: A Tensor with shape [n_main, n_features], where each row
        contains the sum of features over a trajectory generated by an example
        w_proxy.
      aux_input: An Tensor with shape [n_aux, n_features], where each row
        contains the sum of features over a trajectory generated by a randomly
        sampled w_proxy.
      n_main: A scalar Tensor whose value matches the first dimension of
          main_input. Kinda pesky, but necessary for splitting combined_output
          later.
      n_aux: A scalar Tensor whose value matches the first dimenstion of
          aux_input.
      layer_out_sizes: A list containing the output sizes of each dense layer
          of the reward prediction network.
      sampler: A Sampler object.
      is_training: A boolean Tensor indicating whether the network is in
        training mode.
      batch_norm: A boolean indicating whether or not we should apply batch_norm
        to the network.
      layer_collections: A LayerCollections object, used to store activation
        inputs and outputs of each network layer and to update the Fisher
        matrix.
      n_particles: A scalar Tensor object indicating the number of neural
        networks to sample from when generating outputs. (Drawn from Baysian
        Neural Network distribution)
    Returns:
      main_output: A Tensor with shape [n_main, n_particles], where the ith row
        contains `particle` samples of reward predictions for a trajectory with
        the accumulated features `main_input[i]`.
      aux_output: A Tensor with shape [n_aux, n_particles], where the ith row
        contains `particle` samples of reward predictions for a trajectory with
        the accumulated features `aux_input[i]`.
      aux_output_lse: A Tensor with shape [n_particles], containing the
        log-sum-exp over `aux_output`, which is the log of the estimated IRD
        normalizer Z(w_star) for each of the sampled neural networks.
      ll_sep: A Tensor with shape [n_main, n_particles], where (i, j)-th entry
        is the estimated log-likelihood of any w_proxy producing the trajectory
        features `main_output[i]`, given the true reward is the network
        parameterized by the `j`th particle.
      ll: A scalar Tensor containing the average log likelihood over all the
        the trajectories and particles in `ll_sep`.
    """
    with tf.variable_scope("ird_traj"):
        assert_n_main_in = tf.assert_equal(n_main, tf.shape(main_input)[0],
                name="assert_n_main_input")
        assert_n_aux_in = tf.assert_equal(n_aux, tf.shape(aux_input)[0],
                name="assert_n_aux_input")

        # Combine main and aux inputs into a single input Tensor.
        with tf.control_dependencies([assert_n_main_in, assert_n_aux_in]):
            input_combined = tf.concat(main_input, aux_input,
                    name="input_combined")

        output_combined, l2_loss = fully_connected(input_combined, sampler,
                is_training, batch_norm, layer_collection, n_particles,
                layer_out_sizes)
        output_combined = tf.identity(output_combined, name="output_combined")

        # Split output_combined back into main and aux components.
        main_output, aux_output = tf.split(output_combined, [n_main])
        main_output = tf.identity(main_output, name="main_output")
        aux_output = tf.identity(aux_output, name="aux_output")

        # Sanity check to ensure that splitting works here. I can remove later.
        main_shape = tf.shape(main_output)
        aux_shape = tf.shape(aux_output)
        assert_n_main_out = tf.assert_equal(n_main, main_shape[0],
                name="assert_n_main_output")
        assert_n_aux_out = tf.assert_equal(n_aux, aux_shape[0],
                name="assert_n_aux_output")
        assert_n_main_particles = tf.assert_equal(n_particles, main_shape[2],
                name="assert_n_main_particles"))
        assert_n_aux_particles = tf.assert_equal(n_particles, aux_shape[2],
                name="assert_n_aux_particles"))
        with tf.control_dependencies([assert_n_main_out, assert_n_aux_out,
            assert_n_aux_particles, assert_n_main_particles]):
            # Calculate normalization constant for each particle.
            aux_output_lse = tf.reduce_log_sum_exp(aux_output,
                    name="aux_output_lse")
            ll_sep = tf.subtract(main_output, aux_output_lse, name="ll_sep")
            ll = tf.reduce_mean(ll_sep, name="ll")
            loss = tf.identity(-ll, name="loss")

    return main_output, aux_output, aux_output_lse, ll_sep, ll


def fully_connected(inputs, sampler, is_training, batch_norm,
        layer_collection, particles, out_sizes):
    """
    out_sizes (list[int]): A nonempty list of layer output sizes. For example,
      `[10, 1]` describes a fully connected network with a scalar output
      and a single hidden layer of size 10. Input size is inferred from
      `inputs`.
    Returns:
      pred: A Tensor with shape [n_inputs, particles] providing `particles`
        samples of outputs for each row of inputs.
      l2_loss: The l2 regularization error, equal to the sum of squares of
        weights in this neural network.
    """
    def FCBlock(inputs, aux_inputs, out_channel, layer_idx,
            ignore_activation=False):
        with tf.variable_scope("layer"+layer_idx):
            in_channel = inputs.shape.as_list()[-1]
            sampler.register_block(layer_idx, (in_channel, out_channel))
            weights = sampler.sample(layer_idx)
            l2_loss = 0.5 * tf.reduce_sum(weights ** 2)

            # True outputs
            pre, act = dense(inputs, weights, batch_norm, is_training,
                    particles)
            layer_collection.register_fully_connected(
                    sampler.get_params(layer_idx), inputs, pre)
            output = tf.identity(pre if ignore_activation else act,
                    name="output")

            return output, l2_loss

    with tf.variable_scope("fully_connected"):
        inputs = tf.tile(inputs, [particles, 1])
        prev_inputs, l2_loss = inputs, 0.

        for i, out_size in enumerate(out_sizes):
            # The final output is unactivated.
            ignore_activation = (i == len(out_sizes) - 1)
            prev_inputs, prev_aux_inputs, loss = FCBlock(prev_inputs,
                    out_size, i, ignore_activation)
            l2_loss += loss

        pred = tf.identity(prev_inputs, name="pred")
        l2_loss = tf.identity(l2_loss, name="l2_loss")
        layer_collection.register_normal_predictive_distribution(pred)

    return pred, l2_loss

def _build_fc_ird(out_sizes):
    def fc(*args, **kwargs):
        return FullyConnected(*args, out_sizes=out_sizes, ird=True, **kwargs)
    return fc
