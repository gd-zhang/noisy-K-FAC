import tensorflow as tf
import numpy as np
import functools

from ..misc.layers import dense
from .registry import register_model
from ..ops import weight_blocks

def IRDFeat(main_input_traj, aux_input_traj, n_main, n_aux, n_timesteps,
        n_features, layer_out_sizes, sampler,
        is_training, batch_norm, n_particles, scope=None):
    """
    Build a IRD-style dense network which predicts `main_outputs_traj`, the
    rewards of features of state-action pairs in `main_input_traj`. Similarly,
    IRDFeat predicts `aux_outputs_traj`, the reward of trajectory features in
    `aux_input_traj`.

    During the training process, `main_input_traj` should be populated with the
    features generated by all the state-action pairs of trajectories executed
    according to w_proxy. `aux_input_traj` should be populated with the
    features associated with state-action pairs generated from trajectories of
    randomly sampled proxy rewards.

    During testing, we wish to evaluate `main_outputs` given `main_input`.
    `aux_input` can be set to an empty rank 2 vector, and `n_aux` to 0.

    `main_outputs` represent the unnormalized rewards.
    `aux_outputs` are used to compute the normalization constant Z(w_star).

    This function also constructs `ll`, the mean approximate IRD log-likelihood
    of any proxy reward weights that could produce `main_input`, given that the
    currently sampled weights of the neural network are the true reward weights.
    The mean is taken over all the feature vectors in `main_input` and each of
    the `n_particles` weights drawn from the neural network.

    Params:
      main_input_traj: A Tensor with shape [n_main, n_timesteps, n_features],
        where each row contains the sum of features over a trajectory generated
        by an example w_proxy.
      aux_input_traj: An Tensor with shape [n_aux, n_timesteps, n_features],
        where each row
        contains the sum of features over a trajectory generated by a randomly
        sampled w_proxy.
      n_main: A scalar Tensor whose value matches the first dimension of
        main_input. Kinda pesky, but necessary for splitting combined_output
        later.
      n_aux: A scalar Tensor whose value matches the first dimension of
        aux_input.
      n_timesteps: A scalar Tensor whose value matches the second dimensions of
        main_input and aux_input. Conceptually, this is the time horizon of each
        trajectory.
      n_features: A scalar Tensor whose value matches the third dimensions of
        main_input and aux_input.
      layer_out_sizes: A list containing the output sizes of each dense layer
          of the reward prediction network.
      sampler: A Sampler object.
      is_training: A boolean Tensor indicating whether the network is in
        training mode.
      batch_norm: A boolean indicating whether or not we should apply batch_norm
        to the network.
      n_particles: A scalar Tensor object indicating the number of neural
        networks to sample from when generating outputs. (Drawn from Baysian
        Neural Network distribution)
      scope: A scope name for the operations and Variables generated by this
        function.
    Returns:
      main_output: A Tensor with shape [n_particles, n_main], where the ith row
        contains `particle` samples of reward predictions for a trajectory with
        the accumulated features `main_input[i]`.
      aux_output: A Tensor with shape [n_particles, n_aux], where the ith row
        contains `particle` samples of reward predictions for a trajectory with
        the accumulated features `aux_input[i]`.
      aux_output_lse: A Tensor with shape [n_particles], containing the
        log-sum-exp over `aux_output`, which is the log of the estimated IRD
        normalizer Z(w_star) for each of the sampled neural networks.
      ll_sep: A Tensor with shape [n_particles, n_main], where (i, j)-th entry
        is the estimated log-likelihood of any w_proxy producing the trajectory
        features `main_output[i]`, given the true reward is the network
        parameterized by the `j`th particle.
      ll: A scalar Tensor containing the average log likelihood over all the
        the trajectories and particles in `ll_sep`.
      l2_loss: A Tensor with shape [n_particles] containing the sum of squares
        of weights of each particle.
      assert_group: A tf.group of tf.Assert operations that act as
        sanity checks on this network.
      param_dict: A dictionary {sample => (mean, fisher_diag)}. The keys are
        all the weight_samples with which we want to take Noisy Adam gradients
        to. The values are the variational parameters we need to update for
        each weight.
    """
    # Might want to move this scope up a level to outside the function call.
    with tf.variable_scope(name_or_scope=scope, default_name="ird_feat"):
        assert_list = []
        assert_list.append(tf.assert_equal(
                [n_main, n_timesteps, n_features], tf.shape(main_input_traj),
                name="assert_shape_main_input"))
        assert_list.append(tf.assert_equal(
                [n_aux, n_timesteps, n_features], tf.shape(aux_input_traj),
                name="assert_shape_aux_input"))

        # Flatten timesteps to get non-traj inputs
        main_input = tf.reshape(main_input_traj,
                [n_main*n_timesteps, n_features], name="main_input")
        aux_input = tf.reshape(aux_input_traj,
                [n_aux*n_timesteps, n_features], name="aux_input")

        # Build dense network.
        main_output, aux_output, l2_loss, param_dict = fully_connected(
                main_input, aux_input,
                sampler, is_training, batch_norm, n_particles,
                layer_out_sizes)

        main_output = tf.identity(main_output, name="main_output")
        aux_output = tf.identity(aux_output, name="aux_output")

        # Sanity check to ensure that splitting works here. I can remove later.
        main_shape = tf.shape(main_output)
        aux_shape = tf.shape(aux_output)
        assert_list.append(tf.assert_equal(
                [n_particles, n_main * n_timesteps], tf.shape(main_output),
                name="assert_shape_main_output"))
        assert_list.append(tf.assert_equal(
                [n_particles, n_aux * n_timesteps], tf.shape(aux_output),
                name="assert_shape_aux_output"))

        # Reshape to group individual trajectories before summing rewards.
        main_output_traj_presum = tf.reshape(main_output,
                [n_particles, n_main, n_timesteps],
                name="main_output_traj_presum")
        aux_output_traj_presum = tf.reshape(aux_output,
                [n_particles, n_aux, n_timesteps],
                name="aux_output_traj_presum")

        # Sum over timesteps axis to get total trajectory rewards.
        main_output_traj = tf.reduce_sum(main_output_traj_presum, axis=2,
                name="main_output_traj")
        aux_output_traj = tf.reduce_sum(aux_output_traj_presum, axis=2,
                name="aux_output_traj")

        # Sanity check for trajectory rewards shape.
        assert_list.append(tf.assert_equal(
                [n_particles, n_main], tf.shape(main_output_traj),
                name="asssert_shape_main_out_traj"))
        assert_list.append(tf.assert_equal(
                [n_particles, n_aux], tf.shape(aux_output_traj),
                name="assert_shape_aux_out_traj"))

        # Calculate normalization constant for each particle.
        aux_output_lse = tf.reduce_logsumexp(aux_output_traj,
                name="aux_output_lse_pre", axis=1)

        # Sanity check on shape of aux_output_lse.
        assert_list.append(tf.assert_equal(
                [n_particles], tf.shape(aux_output_lse),
                name="assert_shape_aux_output_lse"))

        # Calculate log-likelihood of a proxy reward generating each
        # trajectory, given each of the sampled neural network weights.
        ll_sep = tf.subtract(main_output_traj, aux_output_lse, name="ll_sep")

        # Sanity check on shape of ll_sep
        assert_list.append(tf.assert_equal(
                [n_particles, n_main], tf.shape(aux_output_lse),
                name="assert_shape_ll_sep"))

        # Average log-likelihood over all trajectories and all neural
        # networks.
        ll = tf.reduce_mean(ll_sep, name="ll")

        loss = tf.identity(-ll, name="loss")

        assert_group = tf.group(*assert_list)

    return (main_output, aux_output, aux_output_lse, ll_sep, ll, l2_loss,
            assert_group, param_dict)


def fully_connected(main_inputs, aux_inputs, sampler, is_training, batch_norm,
        particles, out_sizes):
    """
    out_sizes (list[int]): A nonempty list of layer output sizes. For example,
      `[10, 1]` describes a fully connected network with a scalar output
      and a single hidden layer of size 10. Input size is inferred from
      `inputs`.
    Returns:
      main_pred: A Tensor with shape [n_main_inputs, particles] providing
        `particles` samples of outputs for each row of inputs.
      aux_pred: A Tensor with shape [n_aux_inputs, particles] providing
        `particles` samples of outputs for each row of inputs.
      l2_loss: The l2 regularization error, equal to the one-half times the sum
        of squares of weights in this neural network. This is useful for
        noisy-kfac because its gradient is equal to the weight vector itself.
      param_dict: A dictionary mapping weight_samples to their variational
        parameters (mean, fisher_diag).
    """
    param_dict = dict()
    def FCBlock(main_inputs, aux_inputs, out_channel, layer_idx,
            ignore_activation=False):
        with tf.variable_scope("layer"+str(layer_idx)):
            in_channel = main_inputs.shape.as_list()[-1]
            sampler.register_block(layer_idx, (in_channel, out_channel))
            # TODO: Make sure that this assert is satisfied... how do we make
            # sure that this sort of block is actually used?
            assert all([isinstance(b, weight_blocks.FFG_IRDBlock)
                    for b in sampler.get_block()])
            weights = sampler.sample(layer_idx)
            param_dict[weights] = sampler.get_params(layer_idx)
            l2_loss = 0.5 * tf.reduce_sum(weights ** 2)

            # Main outputs
            pre, act = dense(main_inputs, weights, batch_norm, is_training,
                    particles)
            main_output = tf.identity(pre if ignore_activation else act,
                    name="main_out")

            # Aux outputs:
            pre, act = dense(aux_inputs, weights, batch_norm, is_training,
                    particles)
            aux_output = tf.identity(pre if ignore_activation else act,
                    name="aux_out")

            return main_output, aux_output, l2_loss

    with tf.variable_scope("fully_connected"):
        main_inputs = tf.tile(main_inputs, [particles, 1])
        aux_inputs = tf.tile(aux_inputs, [particles, 1])
        prev_main_inputs, prev_aux_inputs, l2_loss = main_inputs, aux_inputs, 0.

        for i, out_size in enumerate(out_sizes):
            # The final output is unactivated.
            ignore_activation = (i == len(out_sizes) - 1)

            prev_main_inputs, prev_aux_inputs, loss = FCBlock(prev_main_inputs,
                    prev_aux_inputs, out_size, i, ignore_activation)
            l2_loss += loss

        main_pred = tf.identity(prev_main_inputs, name="main_pred")
        aux_pred = tf.identity(prev_aux_inputs, name="aux_pred")
        l2_loss = tf.identity(l2_loss, name="l2_loss")

    return main_pred, aux_pred, l2_loss, param_dict

def _build_ird(layer_out_sizes):
    return functools.partial(IRDFeat, layer_out_sizes=layer_out_sizes)

register_model("ird0_1")(_build_ird([1]))
register_model("ird1_10")(_build_ird([10, 1]))
register_model("ird2_10")(_build_ird([10, 10, 1]))
register_model("ird2_20")(_build_ird([20, 20, 1]))
