import tensorflow as tf
import numpy as np

from ..core import MODE_REGRESSION, MODE_IRD
from ..misc.layers import dense
from .registry import register_model

def IRDFeat(main_inputs_traj, aux_inputs_traj, n_main, n_aux, n_timesteps,
        layer_out_sizes, sampler,
        is_training, batch_norm, layer_collections, n_particles, scope=None):
    """
    Build a IRD-style dense network which predicts `main_outputs_traj`, the
    rewards of features of state-action pairs in `main_inputs_traj`. Similarly,
    IRDFeat predicts `aux_outputs_traj`, the reward of trajectory features in
    `aux_inputs_traj`.

    During the training process, `main_inputs_traj` should be populated with the
    features generated by all the state-action pairs of trajectories executed
    according to w_proxy. `aux_inputs_traj` should be populated with the
    features associated with state-action pairs generated from trajectories of
    randomly sampled proxy rewards.

    During testing, we wish to evaluate `main_outputs` given `main_inputs`.
    `aux_inputs` can be set to an empty rank 2 vector, and `n_aux` to 0.

    `main_outputs` represent the unnormalized rewards.
    `aux_outputs` are used to compute the normalization constant Z(w_star).

    This function also constructs `ll`, the mean approximate IRD log-likelihood
    of any proxy reward weights that could produce `main_inputs`, given that the
    currently sampled weights of the neural network are the true reward weights.
    The mean is taken over all the feature vectors in `main_inputs` and each of
    the `n_particles` weights drawn from the neural network.

    Params:
      main_input: A Tensor with shape [n_main, n_timesteps, n_features], where\
        each row contains the sum of features over a trajectory generated by an
        example w_proxy.
      aux_input: An Tensor with shape [n_aux, n_features], where each row
        contains the sum of features over a trajectory generated by a randomly
        sampled w_proxy.
      n_main: A scalar Tensor whose value matches the first dimension of
        main_input. Kinda pesky, but necessary for splitting combined_output
        later.
      n_aux: A scalar Tensor whose value matches the first dimension of
        aux_input.
      n_timesteps: A scalar Tensor whose value matches the second dimensions of
        main_input and aux_input. Conceptually, this is the time horizon of each
        trajectory.
      n_features: A scalar Tensor whose value matches the third dimensions of
        main_input and aux_input.
      layer_out_sizes: A list containing the output sizes of each dense layer
          of the reward prediction network.
      sampler: A Sampler object.
      is_training: A boolean Tensor indicating whether the network is in
        training mode.
      batch_norm: A boolean indicating whether or not we should apply batch_norm
        to the network.
      layer_collections: A LayerCollections object, used to store activation
        inputs and outputs of each network layer and to update the Fisher
        matrix.
      n_particles: A scalar Tensor object indicating the number of neural
        networks to sample from when generating outputs. (Drawn from Baysian
        Neural Network distribution)
      scope: A scope name for the operations and Variables generated by this
        function.
    Returns:
      main_output: A Tensor with shape [n_main, n_particles], where the ith row
        contains `particle` samples of reward predictions for a trajectory with
        the accumulated features `main_input[i]`.
      aux_output: A Tensor with shape [n_aux, n_particles], where the ith row
        contains `particle` samples of reward predictions for a trajectory with
        the accumulated features `aux_input[i]`.
      aux_output_lse: A Tensor with shape [n_particles], containing the
        log-sum-exp over `aux_output`, which is the log of the estimated IRD
        normalizer Z(w_star) for each of the sampled neural networks.
      ll_sep: A Tensor with shape [n_main, n_particles], where (i, j)-th entry
        is the estimated log-likelihood of any w_proxy producing the trajectory
        features `main_output[i]`, given the true reward is the network
        parameterized by the `j`th particle.
      ll: A scalar Tensor containing the average log likelihood over all the
        the trajectories and particles in `ll_sep`.
      assert_group: A tf.group of tf.Assert operations that act as
          sanity checks on this network.
    """
    # Might want to move this scope up a level to outside the function call.
    with tf.variable_scope(name=scope, default_name="ird_feat"):
        assert_list = []
        assert_list.append(tf.assert_equal(
                [n_main, n_timesteps, n_features], tf.shape(main_input_traj),
                name="assert_shape_main_input"))
        assert_list.append(tf.assert_equal(
                [n_aux, n_timesteps, n_features], tf.shape(aux_input_traj),
                name="assert_shape_aux_input"))

        # Flatten timesteps to get non-traj inputs
        main_input = tf.reshape(main_input_traj,
                [n_main*n_timesteps, n_features], name="main_input")
        aux_input = tf.reshape(aux_input_traj,
                [n_aux*n_timesteps, n_features], name="aux_input")


        # Combine main and aux inputs into a single input Tensor.
        input_combined = tf.concat(main_input, aux_input,
                name="input_combined")

        # Build dense network.
        output_combined, l2_loss = fully_connected(input_combined, sampler,
                is_training, batch_norm, layer_collection, n_particles,
                layer_out_sizes)

        output_combined = tf.identity(output_combined, name="output_combined")

        # Split output_combined back into main and aux components.
        main_output, aux_output = tf.split(output_combined,
                [n_main*n_timesteps, n_aux*n_timesteps])
        main_output = tf.identity(main_output, name="main_output")
        aux_output = tf.identity(aux_output, name="aux_output")

        # Sanity check to ensure that splitting works here. I can remove later.
        main_shape = tf.shape(main_output)
        aux_shape = tf.shape(aux_output)
        assert_list.append(tf.assert_equal(
                [n_main * n_timesteps, n_particles], tf.shape(main_output),
                name="assert_shape_main_output"))
        assert_list.append(tf.assert_equal(
                [n_aux * n_timesteps, n_particles], tf.shape(aux_output),
                name="assert_shape_aux_output"))

        # Reshape to group individual trajectories before summing rewards.
        main_output_traj_presum = tf.reshape(main_output,
                [n_main, n_timesteps, n_particles],
                name="main_output_traj_presum")
        aux_output_traj_presum = tf.reshape(aux_output,
                [n_aux, n_timesteps, n_particles],
                name="aux_output_traj_presum")

        # Sum over timesteps axis to get total trajectory rewards.
        main_output_traj = tf.reduce_sum(main_output_traj_presum, axis=1,
                name="main_output_traj")
        aux_output_traj = tf.reduce_sum(aux_output_traj_presum, axis=1,
                name="aux_output_traj")

        # Sanity check for trajectory rewards shape.
        assert_list.append(tf.assert_equal(
                [n_main * n_particles], tf.shape(main_output_traj),
                name="asssert_shape_main_out_traj"))
        assert_list.append(tf.assert_equal(
                [n_aux * n_particles], tf.shape(aux_output_traj),
                name="assert_shape_aux_out_traj"))

        # Calculate normalization constant for each particle.
        aux_output_lse = tf.reduce_log_sum_exp(aux_output_traj,
                name="aux_output_lse_pre", axis=0)

        # Sanity check on shape of aux_output_lse.
        assert_list.append(tf.assert_equal(
                [n_particles], tf.shape(aux_output_lse),
                name="assert_shape_aux_output_lse"))

        # Calculate log-likelihood of a proxy reward generating each
        # trajectory, given each of the sampled neural network weights.
        ll_sep = tf.subtract(main_output, aux_output_lse, name="ll_sep")

        # Average log-likelihood over all trajectories and all neural
        # networks.
        ll = tf.reduce_mean(ll_sep, name="ll")

        # Sanity check on shape of ll.
        assert_list.append(tf.assert_equal([], tf.shape(ll),
            name="assert_shape_ll"))

        loss = tf.identity(-ll, name="loss")

        assert_group = tf.group(*assert_list)

    return main_output, aux_output, aux_output_lse, ll_sep, ll, assert_group


def fully_connected(inputs, sampler, is_training, batch_norm,
        layer_collection, particles, out_sizes):
    """
    out_sizes (list[int]): A nonempty list of layer output sizes. For example,
      `[10, 1]` describes a fully connected network with a scalar output
      and a single hidden layer of size 10. Input size is inferred from
      `inputs`.
    Returns:
      pred: A Tensor with shape [n_inputs, particles] providing `particles`
        samples of outputs for each row of inputs.
      l2_loss: The l2 regularization error, equal to the sum of squares of
        weights in this neural network.
    """
    def FCBlock(inputs, aux_inputs, out_channel, layer_idx,
            ignore_activation=False):
        with tf.variable_scope("layer"+layer_idx):
            in_channel = inputs.shape.as_list()[-1]
            sampler.register_block(layer_idx, (in_channel, out_channel))
            weights = sampler.sample(layer_idx)
            l2_loss = 0.5 * tf.reduce_sum(weights ** 2)

            # True outputs
            pre, act = dense(inputs, weights, batch_norm, is_training,
                    particles)
            layer_collection.register_fully_connected(
                    sampler.get_params(layer_idx), inputs, pre)
            output = tf.identity(pre if ignore_activation else act,
                    name="output")

            return output, l2_loss

    with tf.variable_scope("fully_connected"):
        inputs = tf.tile(inputs, [particles, 1])
        prev_inputs, l2_loss = inputs, 0.

        for i, out_size in enumerate(out_sizes):
            # The final output is unactivated.
            ignore_activation = (i == len(out_sizes) - 1)
            prev_inputs, prev_aux_inputs, loss = FCBlock(prev_inputs,
                    out_size, i, ignore_activation)
            l2_loss += loss

        pred = tf.identity(prev_inputs, name="pred")
        l2_loss = tf.identity(l2_loss, name="l2_loss")
        layer_collection.register_normal_predictive_distribution(pred)

    return pred, l2_loss

def _build_fc_ird(out_sizes):
    def fc(*args, **kwargs):
        return FullyConnected(*args, out_sizes=out_sizes, ird=True, **kwargs)
    return fc
